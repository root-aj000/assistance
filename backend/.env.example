# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# Embedding Configuration
# Set to true to use FREE local embeddings (no API calls, unlimited usage)
# Set to false to use Gemini embeddings (requires API quota)
USE_LOCAL_EMBEDDINGS=true
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Gemini Model Selection
# Available models: gemini-2.5-flash, gemini-2.5-pro, gemini-2.5-flash-lite
GEMINI_FLASH_MODEL=gemini-2.5-flash
GEMINI_PRO_MODEL=gemini-2.5-pro
GEMINI_LITE_MODEL=gemini-2.5-flash-lite
GEMINI_EMBEDDING_MODEL=models/text-embedding-004

# Default model to use for chat
DEFAULT_CHAT_MODEL=gemini-2.5-flash

# Model-specific Rate Limiting (requests per minute)
# Set to 3 RPM to avoid quota issues on free tier
FLASH_RATE_LIMIT_RPM=3
PRO_RATE_LIMIT_RPM=3
LITE_RATE_LIMIT_RPM=3
EMBEDDING_RATE_LIMIT_RPM=3

# Vector Database (FAISS)
VECTOR_DB_PATH=./data/vector_db

# Graph Database (Neo4j)
GRAPH_DB_URL=bolt://localhost:7687
GRAPH_DB_USER=neo4j
GRAPH_DB_PASSWORD=your_neo4j_password_here

# Token Management
MAX_TOKENS_PER_REQUEST=70000
SYSTEM_PROMPT_RESERVE=3000

# Legacy Rate Limiting (kept for backwards compatibility)
RATE_LIMIT_RPM=3
RATE_LIMIT_TPM=100000

# Server Configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=5001

# Frontend Configuration
FRONTEND_URL=http://localhost:3000

# Indexing Configuration
CHUNK_SIZE_TOKENS=400
CHUNK_OVERLAP=50

# Repository to Auto-Index on Startup (Optional)
# Set this to automatically index your codebase when the backend starts
# Example Windows path: C:/path/to/your/repo
# Example Linux/Mac path: /path/to/your/repo
REPOSITORY_PATH=

